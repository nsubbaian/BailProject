{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BailProject-WebScraping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyORvWbmNos9oCSzziWfD57x"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uy9cFYjpbkBv"
      },
      "source": [
        "#url = 'https://raw.githubusercontent.com/vera-institute/incarceration-trends/master/incarceration_trends.csv'\n",
        "# df_incarceration = pd.read_csv(url, error_bad_lines=False)\n",
        "\n",
        "# url = 'https://raw.githubusercontent.com/vera-institute/incarceration-trends/master/incarceration_trends_jail_jurisdiction.csv'\n",
        "# df_jailjurisdiction = pd.read_csv(url, error_bad_lines=False)\n",
        "\n",
        "# df_incarceration[df_incarceration['county_name'] == 'Oktibbeha County'][df_incarceration['state'] == 'MS']\n",
        "# np.set_printoptions(threshold=sys.maxsize)\n",
        "# print(np.unique(df_incarceration['county_name'].values))\n",
        "# print(df_incarceration.columns.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ct3OOaFgPdo"
      },
      "source": [
        "Harrison county Jail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCRxhuoF9rJE",
        "outputId": "b473b60e-a62f-4d01-cb21-d5f67498c2eb"
      },
      "source": [
        "import requests\n",
        "from IPython import display\n",
        "from base64 import b64decode\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "from google.colab import output\n",
        "from datetime import datetime, date\n",
        "from pytz import timezone\n",
        "import threading\n",
        "import requests\n",
        "from requests.structures import CaseInsensitiveDict\n",
        "import runpy\n",
        "import webbrowser\n",
        "import urllib.request\n",
        "import math\n",
        "from bs4 import BeautifulSoup as soup\n",
        "import bs4\n",
        "from urllib.request import urlopen as uReq\n",
        "from lxml import html\n",
        "import json\n",
        "from re import sub\n",
        "from decimal import Decimal\n",
        "import csv\n",
        "\n",
        "!pip install --force https://github.com/chengs/tqdm/archive/colab.zip\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[?25l  Downloading https://github.com/chengs/tqdm/archive/colab.zip\n",
            "\u001b[K     | 194kB 1.4MB/s\n",
            "\u001b[?25hBuilding wheels for collected packages: tqdm\n",
            "  Building wheel for tqdm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tqdm: filename=tqdm-4.28.1-py2.py3-none-any.whl size=47879 sha256=4344b1cc2ead203755c130ea59c7bef24f42a52a54a9b9c36a543ecf92539c3d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nqwuw84l/wheels/41/18/ee/d5dd158441b27965855b1bbae03fa2d8a91fe645c01b419896\n",
            "Successfully built tqdm\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.28.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.28.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tqdm\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed tqdm-4.28.1\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLLFpjTJmiY9",
        "outputId": "b4f83d96-04fd-4a3c-f84a-41342faab65c"
      },
      "source": [
        "today_date = datetime.now(timezone('US/Eastern')).strftime(\"%m-%d-%Y\")\n",
        "# today_date = '06-12-2021'\n",
        "# today_date = \"TEST\"\n",
        "os.chdir('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/')\n",
        "RAW_DIR = 'RAW/' +  today_date\n",
        "os.mkdir(RAW_DIR)\n",
        "CLEAN_DIR = 'CLEAN/' +  today_date\n",
        "os.mkdir(CLEAN_DIR)\n",
        "print(\"Date used for scraping:\", today_date)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Date used for scraping: TEST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gCwGWStsTpy",
        "cellView": "form"
      },
      "source": [
        "#@markdown Townscraper Script\n",
        "\n",
        "# ---------------------- Towns ----------------------\n",
        "jail_captcha = {\n",
        "                'DeSoto': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/DeSoto_County_Ms/'},\n",
        "                'Forrest': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Forrest_County_MS/'},\n",
        "                'Hancock': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/HANCOCK_COUNTY_MS/'},\n",
        "                'Harrison': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/HARRISON_COUNTY_JAIL_MS/'},\n",
        "                'Lamar': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Lamar_County_MS/'},\n",
        "                'Marion': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Marion_County_MS/'},\n",
        "                'Perry': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Perry_County_MS/'},\n",
        "                'Yazoo': {'URL': 'https://omsweb.public-safety-cloud.com/jtclientweb/Offender/Yazoo_County_MS/'}\n",
        "                }\n",
        "                \n",
        "def townScraper(town, validate_r): \n",
        "  print(\"County: \", town)\n",
        "  captchaKey_aftervalidation = validate_r.json()['captchaKey']\n",
        "\n",
        "  # Get offender information\n",
        "  records_r = requests.post(\n",
        "      jail_captcha[town]['URL'],\n",
        "      json={'captchaKey':validate_r.json()['captchaKey']}\n",
        "  )\n",
        "  offenderViewKey = records_r.json()['offenderViewKey']\n",
        "  total = len(records_r.json()['offenders'])\n",
        "  print(town, \"No. of offenders:\", total)\n",
        "\n",
        "  # Loop through information and store in Dataframe\n",
        "  inmates = {}\n",
        "  RAW_inmates = {}\n",
        "\n",
        "  for offender in records_r.json()['offenders']:\n",
        "    inmates[offender['arrestNo']] = {}\n",
        "    inmates[offender['arrestNo']][\"Arrest Number\"] = offender['arrestNo']\n",
        "    for j in ['firstName', 'lastName', 'agencyName', 'originalBookDateTime' ]:\n",
        "      inmates[offender['arrestNo']][j] = offender[j]\n",
        "    \n",
        "    RAW_inmates[offender['arrestNo']] = {}\n",
        "    RAW_inmates[offender['arrestNo']][\"Arrest Number\"] = offender['arrestNo']\n",
        "    RAW_inmates[offender['arrestNo']]['offenders'] = offender\n",
        "  \n",
        "\n",
        "  df = pd.DataFrame(columns= [\"Arrest Number\", 'firstName','lastName', 'agencyName', 'originalBookDateTime',           \n",
        "                              'bondAmount', 'bondType', 'chargeDescription', 'chargeStatus',\n",
        "                              'crimeType', 'Bond Total Amount', 'charges', 'cases'])\n",
        "  RAW_df = pd.DataFrame(columns= [\"Arrest Number\", 'charges', 'cases'])\n",
        "  \n",
        "  arrestNos = {}\n",
        "  for arrestNo in list(inmates.keys()):\n",
        "    arrestNos[str(arrestNo)] = {}\n",
        "    arrestNos[str(arrestNo)][\"Number\"] = arrestNo\n",
        "    URL = jail_captcha[town]['URL'] + str(arrestNo) + '/offenderbucket/' + str(offenderViewKey)\n",
        "    response = requests.post(URL, json={'captchaImage': image,'captchaKey': captchaKey_aftervalidation})\n",
        "\n",
        "    # Iterate through charges in response\n",
        "    bond_total = 0\n",
        "    for column in ['charges', 'cases','bondType', 'bondAmount', 'chargeDescription', 'chargeStatus', 'crimeType']:\n",
        "      inmates[str(arrestNo)][column] = []\n",
        "\n",
        "    RAW_inmates[str(arrestNo)]['charges'] =[]\n",
        "    for charge in response.json()['charges']:\n",
        "      # print(charge)\n",
        "      inmates[str(arrestNo)]['charges'].append(charge)\n",
        "      RAW_inmates[str(arrestNo)]['charges'].append(charge)\n",
        "      bondAmt = charge['bondAmount']\n",
        "      if bondAmt == None: \n",
        "        bondAmt = 0\n",
        "      bond_total = bond_total + float(bondAmt)\n",
        "      for column in ['bondType', 'bondAmount', 'chargeDescription', 'chargeStatus', 'crimeType']:\n",
        "        inmates[str(arrestNo)][column].append(charge[column])\n",
        "\n",
        "    inmates[str(arrestNo)]['Bond Total Amount'] = bond_total\n",
        "\n",
        "    inmates[str(arrestNo)]['Potentially Bondable?'] = \"\"\n",
        "    arrestNos[str(arrestNo)][\"Potentially Bondable?\"] = ''\n",
        "    if all((x == \"WRITTEN BOND\" or \n",
        "            x == 'SURETY BOND' or \n",
        "            x == 'OWN RECOGNIZANCE BOND' or\n",
        "            x == 'OFF BOND' or\n",
        "            x == 'SURETY BOND WITH CONDITIONS' or \n",
        "            x == 'SURETY'\n",
        "            )for x in inmates[str(arrestNo)]['bondType']) and len(inmates[str(arrestNo)]['bondType']) >0  : \n",
        "      inmates[str(arrestNo)]['Potentially Bondable?'] = 'Yes'\n",
        "      arrestNos[str(arrestNo)][\"Potentially Bondable?\"] = 'Yes'\n",
        "    \n",
        "    inmates[str(arrestNo)]['Sex/DV charge'] = \"\"\n",
        "    arrestNos[str(arrestNo)][\"Sex/DV charge\"] = \"\"\n",
        "    if any(re.search(\"sex\", str(x), re.IGNORECASE) for x in inmates[str(arrestNo)]['chargeDescription']): \n",
        "      inmates[str(arrestNo)]['Sex/DV charge'] = 'Yes'\n",
        "      arrestNos[str(arrestNo)][\"Sex/DV charge\"] = 'Yes'\n",
        "    \n",
        "    inmates[str(arrestNo)]['Over 5k?'] = \"\"\n",
        "    arrestNos[str(arrestNo)][\"Over 5k?\"] = \"\"\n",
        "    if int(inmates[str(arrestNo)]['Bond Total Amount']) > 5000: \n",
        "      inmates[str(arrestNo)]['Over 5k?'] = 'Yes'\n",
        "      arrestNos[str(arrestNo)][\"Over 5k?\"] = 'Yes'\n",
        "\n",
        "    # iterate through cases in response\n",
        "    RAW_inmates[str(arrestNo)]['cases'] = []\n",
        "    for case in response.json()['cases']:\n",
        "      RAW_inmates[str(arrestNo)]['cases'].append(case)\n",
        "\n",
        "    # iterate through offenderSpecialFields in response\n",
        "    total =[]\n",
        "    for item in response.json()['offenderSpecialFields']:\n",
        "      temp = { }\n",
        "      temp[item['labelText']] = item['offenderValue']\n",
        "      total.append(temp)\n",
        "    RAW_inmates[str(arrestNo)]['offenderSpecialFields'] = total\n",
        "    \n",
        "    # -----------------------------------------------------\n",
        "    # get offenderViewKey for next iteration of for loop  \n",
        "    offenderViewKey = response.json()['offenderViewKey']\n",
        "\n",
        "    # add inmate to dataframe\n",
        "    df = df.append(inmates[str(arrestNo)] , ignore_index=True)\n",
        "    RAW_df = RAW_df.append(RAW_inmates[str(arrestNo)] , ignore_index=True)\n",
        "\n",
        "  # saving inmate info for town to Google Drive\n",
        "  file_name = str(today_date) + \"_\" + town + '_inmates_' + '.csv'\n",
        "  df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/CLEAN/' + today_date + \"/\" + file_name)\n",
        "  print(town, \"saved CLEAN\", file_name)\n",
        "\n",
        "  # saving RAW inmate info for town to Google Drive\n",
        "  file_name = str(today_date) + \"_\" + town + '_inmates_' + '.csv'\n",
        "  RAW_df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/RAW/' + today_date + \"/\" + file_name)\n",
        "  print(town, \"saved RAW\", file_name)\n",
        "\n",
        "  daily_summary = pd.DataFrame(columns= [\"Date\", \"Arrest Numbers\", \"Total arrest Numbers\"])\n",
        "  temp = {}\n",
        "  temp[\"Date\"] = today_date\n",
        "  temp[\"Arrest Numbers\"] = arrestNos\n",
        "  temp[\"Total arrest Numbers\"] = len(arrestNos)\n",
        "  daily_summary = daily_summary.append(temp, ignore_index=True)\n",
        "\n",
        "  try:\n",
        "    daily_summary = pd.read_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/DAILY_SUMMARY/' + str(town) + '.csv').append(daily_summary)\n",
        "  except FileNotFoundError: \n",
        "    print(\"\")\n",
        "\n",
        "  daily_summary = daily_summary[[\"Date\", \"Arrest Numbers\", \"Total arrest Numbers\"]]\n",
        "  daily_summary.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/DAILY_SUMMARY/' + str(town) + '.csv')\n",
        "  print(town, \"saved DAILY SUMMARY for\", town)\n",
        "\n",
        "# Audio notification so you know when to input the captcha code again\n",
        "# output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "I8A4QD_Y49Fz"
      },
      "source": [
        "#@markdown PearlRiver Script\n",
        "def townScraperPearl(): \n",
        "  town = 'PearlRiver'\n",
        "  jail = 'PEARLRIVERMS'\n",
        "\n",
        "  print(\"County:\", town)\n",
        "\n",
        "  headers = CaseInsensitiveDict()\n",
        "  headers[\"Content-Type\"] = \"application/json\"\n",
        "  headers[\"Content-Length\"] = \"0\"\n",
        "  result = requests.post(\"http://inmates.bluhorse.com/Default.aspx/GetPasskey\", headers=headers).json()['d']\n",
        "  keyAns = result.split(\"|||\");\n",
        "  key = keyAns[0]\n",
        "  ans = keyAns[1]\n",
        "\n",
        "  InmateList = 'http://inmates.bluhorse.com/' + \"InmateService.svc/GetInmates2?Jail=\" + jail + \"&key=\" + key + \"&ans=\" + ans\n",
        "  GetInmateBookNo = requests.get(InmateList).json()['GetInmates2Result']\n",
        "  total = len(GetInmateBookNo)\n",
        "  print(town, \"No. of offenders:\", total)\n",
        "\n",
        "  isLogin='false'\n",
        "  rootURL = \"http://inmates.bluhorse.com/InmateService.svc\" \n",
        "\n",
        "  WebFields_url = 'http://inmates.bluhorse.com/' + \"InmateService.svc/GetJailInfo1?Jail=\" + jail + \"&key=\" + key + \"&Answer=\" + ans\n",
        "  WebFields = requests.get(WebFields_url).json()['GetJailInfo1Result']['WebFields']\n",
        "\n",
        "  inmates = {}\n",
        "  df = pd.DataFrame(columns= [\"Arrest Number\", 'firstName','lastName', 'agencyName', 'originalBookDateTime',           \n",
        "                              'bondAmount', 'bondType', 'chargeDescription', 'chargeStatus',\n",
        "                              'crimeType', 'Bond Total Amount', 'charges', 'cases'])\n",
        "  \n",
        "  for offender in GetInmateBookNo:\n",
        "    bookNO = offender['BookNo']\n",
        "\n",
        "    inmates[bookNO] = {}\n",
        "    for column in ['charges', 'cases','bondType', 'bondAmount', 'chargeDescription', 'chargeStatus', 'crimeType']:\n",
        "      inmates[bookNO][column] = []\n",
        "\n",
        "    inmates[bookNO][\"Arrest Number\"] = offender['BookNo']\n",
        "    inmates[bookNO]['firstName'] = offender['FName']\n",
        "    inmates[bookNO]['lastName'] = offender['LName']\n",
        "\n",
        "  for i in GetInmateBookNo:\n",
        "    BookNO = i['BookNo']\n",
        "\n",
        "    GetInmateCharges = requests.get(rootURL + \"/GetInmateCharges?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "    GetInmate = requests.get(rootURL + \"/GetInmate?Jail=\" + jail + \"&bookno=\" + BookNO + \"&key=\" + key + \"&answer=\" + ans + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "    GetInmate_Arrest_Info = requests.get(rootURL + \"/GetInmate_Arrest_Info?Jail=\" + jail + \"&bookno=\" + BookNO + \"&key=\" + key + \"&answer=\" + ans + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "    GetInmateHolds = requests.get(rootURL + \"/GetInmateHolds?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "    GetInmateCourtHistory = requests.get(rootURL+ \"/GetInmateCourtHistory?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "    GetInmateBonds = requests.get(rootURL + \"/GetInmateBonds?Jail=\" + jail + \"&BookNo=\" + BookNO + \"&Fields=\" + WebFields + \"&isLogin=\" + isLogin)\n",
        "\n",
        "    GetInmateChargesResult = GetInmateCharges.json()['GetInmateChargesResult']\n",
        "    GetInmateResult = GetInmate.json()['GetInmateResult']\n",
        "    GetInmate_Arrest_InfoResult = GetInmate_Arrest_Info.json()['GetInmate_Arrest_InfoResult']\n",
        "    GetInmateHoldsResult = GetInmateHolds.json()['GetInmateHoldsResult']\n",
        "    GetInmateCourtHistoryResult = GetInmateCourtHistory.json()['GetInmateCourtHistoryResult']\n",
        "    GetInmateBondsResult = GetInmateBonds.json()['GetInmateBondsResult']\n",
        "    \n",
        "    inmates[BookNO]['agencyName'] = GetInmateResult['ArrestAgency']\n",
        "    inmates[BookNO]['originalBookDateTime'] = GetInmateResult['ArrestDate']\n",
        "    # inmates[BookNO]['cases'] = \n",
        "\n",
        "    bond_total = 0\n",
        "    for charge in GetInmateChargesResult:\n",
        "      inmates[BookNO]['charges'].append(charge)\n",
        "      bondAmt = charge['Bond']\n",
        "      if bondAmt == None: \n",
        "        bondAmt = 0\n",
        "      bond_total = bond_total + float(bondAmt)\n",
        "\n",
        "      # inmates[BookNO]['bondType'].append(charge['ChargeType'])\n",
        "      inmates[BookNO]['bondAmount'].append(charge['Bond'])\n",
        "      inmates[BookNO]['chargeDescription'].append(charge['Description'])\n",
        "      inmates[BookNO]['chargeStatus'].append(charge['Disposition'].strip('\\n').strip('\\r'))\n",
        "      inmates[BookNO]['crimeType'].append(charge['ChargeType'])\n",
        "      \n",
        "      inmates[BookNO]['Bond Total Amount'] = bond_total \n",
        "\n",
        "    inmates[BookNO]['GetInmateChargesResult'] = GetInmateChargesResult\n",
        "    inmates[BookNO]['GetInmateBondsResult'] = GetInmateBondsResult\n",
        "    inmates[BookNO]['GetInmateResult'] = GetInmateResult\n",
        "    inmates[BookNO]['GetInmate_Arrest_InfoResult'] = GetInmate_Arrest_InfoResult\n",
        "    inmates[BookNO]['GetInmateHoldsResult'] = GetInmateHoldsResult\n",
        "    inmates[BookNO]['GetInmateCourtHistoryResult'] = GetInmateCourtHistoryResult\n",
        "\n",
        "    df = df.append(inmates[BookNO] , ignore_index=True)\n",
        "\n",
        "  # saving inmate info for town to Google Drive\n",
        "  file_name = str(today_date) + \"_\" + town + '_inmates_' + '.csv'\n",
        "  df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/CLEAN/' + today_date + \"/\" + file_name)\n",
        "  print(town, \"saved CLEAN\", file_name)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "5mp4WpQWnMkz"
      },
      "source": [
        "#@markdown Clay Script\n",
        "def clay():\n",
        "    #opens the first page of the roster to generate links to inmate pages\n",
        "    roster_prefix = \"http://www.claysheriffms.org/roster.php\"\n",
        "    u_clay = requests.get(roster_prefix)\n",
        "    clay_soup = soup(u_clay.text, \"html.parser\")\n",
        "    u_clay.close()\n",
        "\n",
        "    inmate_prefix = \"http://www.claysheriffms.org/\"\n",
        "    inmate_links, name, bookNo, age, sex, race, arrest_agency, arrest_date, bond, bondable, offenses = ([] for i in range(11))\n",
        "    num_bondable = 0\n",
        "\n",
        "    num_inmates = int(clay_soup.select(\".ptitles\")[0].get_text().split()[2].replace(\"(\", \"\").replace(\")\", \"\"))\n",
        "    num_pages = math.ceil(num_inmates/10)\n",
        "\n",
        "    #steps through the pages of the roster and scrapes for urls to inmate pages\n",
        "    for page in range(num_pages):\n",
        "    \tu_clay = requests.get(roster_prefix + \"?grp=\" + str(page*10))\n",
        "    \tclay_soup = soup(u_clay.text, \"html.parser\")\n",
        "    \tu_clay.close()\n",
        "\n",
        "    \tinmate_table = clay_soup.select(\".inmateTable\")\n",
        "    \tfor i in inmate_table:\n",
        "    \t\tinmate_links.append(inmate_prefix + i.select(\".text2\")[-1].get(\"href\"))\n",
        "\n",
        "    #goes through list of links to inmate pages and finds information\n",
        "    for inmate in inmate_links:\n",
        "    \tu_inmate = requests.get(inmate)\n",
        "    \tinmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "    \tu_inmate.close()\n",
        "\n",
        "    \ttable = inmate_soup.find_all(\"table\")[6].select(\".text2\")\n",
        "    \tname.append(' '.join(inmate_soup.select('.ptitles')[0].get_text().split()))\n",
        "    \tbookNo.append(table[0].get_text().strip())\n",
        "    \tage.append(int(table[1].get_text().strip()))\n",
        "    \tsex.append(table[2].get_text().strip())\n",
        "    \trace.append(table[3].get_text().strip())\n",
        "    \tarrest_agency.append(table[4].get_text().strip())\n",
        "    \tarrest_date.append(table[5].get_text().split()[0])\n",
        "\n",
        "    \tbond_ = float(table[8].get_text().strip(\"$\"))\n",
        "    \tbond.append(bond_)\n",
        "    \tbondable.append(bool(bond_))\n",
        "    \tif (bool(bond_)):\n",
        "    \t\tnum_bondable += 1\n",
        "\n",
        "    \toffenses.append(str(table[7]).strip(\"<span class=\\\"text2\\\">\").strip(\"</\").split(\"<br/>\"))\n",
        "\n",
        "    #generates a dictionary, then creates a dataframe to print\n",
        "    dic = {\"Name\": name, \"Booking number\": bookNo, \"Arrest agency\": arrest_agency, \"Arrest date\": arrest_date,\n",
        "    \"Bondable?\": bondable, \"Bond\": bond, \"Age\": age, \"Sex\": sex, \"Race\": race, \"Offense(s)\": offenses}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/CLEAN/' + today_date + \"/\" + today_date + '_Clay.csv', index=False)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "G0fynMX1znuH"
      },
      "source": [
        "# @markdown Adams Script\n",
        "def adams():\n",
        "    u_Adams = urllib.request.urlopen('http://www.adamscosheriff.org/inmate-roster/')\n",
        "    Adams_html = u_Adams.read()\n",
        "    u_Adams.close()\n",
        "    adams_soup = soup(Adams_html, \"html.parser\")\n",
        "    page_num = adams_soup.select(\".page-numbers\")[-2].get_text()\n",
        "\n",
        "    names, book_Nos,age, gender, race, booking_date, charges, bonds, bondable, bondable_names = ([] for i in range(10))\n",
        "    num_bondable = 0\n",
        "    num_errs = 0\n",
        "\n",
        "    for page in range(int(page_num)):\n",
        "    \tu_page= urllib.request.urlopen('http://www.adamscosheriff.org/inmate-roster/page/' + str(page+1) + \"/\")\n",
        "    \tpage_html = u_page.read()\n",
        "    \tu_page.close()\n",
        "    \tpage_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "    \tpage_profile_links = page_soup.select(\".profile-link\")\n",
        "\n",
        "    \tfor profile in page_profile_links:\n",
        "    \t\ttry:\n",
        "\n",
        "    \t\t\tu_inmate = urllib.request.urlopen(profile.a.get(\"href\"))\n",
        "    \t\t\tinmate_html = u_inmate.read()\n",
        "    \t\t\tu_inmate.close()\n",
        "    \t\t\tinmate_soup = soup(inmate_html, \"html.parser\")\n",
        "\n",
        "    \t\t\tp_vals = inmate_soup.find_all(\"p\")\n",
        "    \t\t\tname = (p_vals[0].get_text().strip(\"Full Name:\").strip())\n",
        "    \t\t\tnames.append(name)\n",
        "    \t\t\tbook_Nos.append((p_vals[1].get_text().strip(\"Booking Number:\").strip()))\n",
        "    \t\t\tage.append(p_vals[2].get_text().strip(\"Age:\").strip())\n",
        "    \t\t\tgender.append(p_vals[3].get_text().strip(\"Gender:\").strip())\n",
        "    \t\t\trace.append(p_vals[4].get_text().strip(\"Race:\").strip())\n",
        "    \t\t\tbooking_date.append(p_vals[7].get_text().strip(\"Booking Date:\").strip())\n",
        "    \t\t\tcharges.append(p_vals[8].get_text().strip(\"Charges:\").strip())\n",
        "    \t\t\tb = p_vals[9].get_text().strip(\"Bond:\").replace(\",\", \"\").strip()\n",
        "    \t\t\tb_able = False\n",
        "    \t\t\tif b == \"\":\n",
        "    \t\t\t\tbond = 0.0\n",
        "    \t\t\telse:\n",
        "    \t\t\t\tbond = float(b)\n",
        "    \t\t\t\tb_able = True\n",
        "    \t\t\t\tnum_bondable += 1\n",
        "    \t\t\t\tbondable_names.append(name)\n",
        "    \t\t\tbonds.append(bond)\n",
        "    \t\t\tbondable.append(b_able)\n",
        "\n",
        "    \t\texcept IndexError:\n",
        "    \t\t\tnum_errs += 1\n",
        "    \t\t\tprint(\"Clay - IndexError\")\n",
        "\n",
        "    dic = {\"Name\": names, \"Booking Number\": book_Nos, \"Age\": age, \"Gender\": gender, \"Race\": race, \"Booking Date\": booking_date,\n",
        "    \t\t\"Charges\": charges, \"Bondable?\": bondable, \"Bond\": bonds}\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/CLEAN/' + today_date + \"/\" + today_date + '_Adams.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "AZ-_t3epzdm0"
      },
      "source": [
        "# @markdown Hinds Script\n",
        "\n",
        "def hinds():\n",
        "    print(\"here\")\n",
        "    #Goes through all pages of the database to find links to the inmate pages\n",
        "    #Is there a way to do this that doesn't open the first page twice? Especially without rewriting a bunch of code outside the loop\n",
        "    u_Hinds = urllib.request.urlopen('http://www.co.hinds.ms.us/pgs/apps/inmate/inmate_list.asp?name_sch=&SS1=1&search_by_city=&search_by=&ScrollAction=Page+1')\n",
        "    Hinds_html = u_Hinds.read()\n",
        "    u_Hinds.close()\n",
        "    hinds_soup = soup(Hinds_html, \"html.parser\")\n",
        "\n",
        "    num_pages = int(hinds_soup.h3.get_text().split()[3])\n",
        "    hinds_prefix = \"http://www.co.hinds.ms.us/pgs/apps/inmate/inmate_list.asp?name_sch=&SS1=1&search_by_city=&search_by=&ScrollAction=Page+\"\n",
        "    inmate_prefix = \"http://www.co.hinds.ms.us/pgs/apps/inmate/\"\n",
        "    inmate_links = []\n",
        "\n",
        "    name, address, dob, sex, race, height, weight, eye_col, hair_col, arrest_agency = ([] for i in range(10))\n",
        "    arrest_date, offense1, offense2, offense3, offense4, pin, location = ([] for i in range(7))\n",
        "\n",
        "    #finds links to all inmate pages\n",
        "    for i in range(num_pages):\n",
        "    \tu_Hinds = urllib.request.urlopen(hinds_prefix + str(i + 1))\n",
        "    \tHinds_html = u_Hinds.read()\n",
        "    \tu_Hinds.close()\n",
        "    \thinds_soup = soup(Hinds_html, \"html.parser\")\n",
        "    \tlin = hinds_soup.find_all(\"a\")\n",
        "\n",
        "    \tlast_link = len(lin) - 6\n",
        "    \tpage_links = lin[6:last_link]\n",
        "\n",
        "    \tfor link in page_links:\n",
        "    \t\tinmate_links.append(inmate_prefix + link.get('href'))\n",
        "\n",
        "    increment = 0\n",
        "    for inmate_link in inmate_links:\n",
        "    \tprint(increment, len(inmate_links))\n",
        "    \ttry:\n",
        "    \t\tu_Hinds = urllib.request.urlopen(inmate_link)\n",
        "    \t\tHinds_html = u_Hinds.read()\n",
        "    \t\tu_Hinds.close()\n",
        "    \t\tinmate_soup = soup(Hinds_html, \"html.parser\")\n",
        "\n",
        "    \t\t#generates a list of all the classes which contain relevant information\n",
        "    \t\tleft_txt = inmate_soup.select(\".normaltxtleft\")\n",
        "\n",
        "    \t\t#generates information available, excludes those which do not seem to have anything listed\n",
        "    \t\tname.append(left_txt[0].get_text().strip())\n",
        "    \t\taddress.append(\" \".join(left_txt[1].get_text().split()))\n",
        "    \t\tdob.append(left_txt[3].get_text().strip())\n",
        "    \t\tsex.append(left_txt[5].get_text().strip())\n",
        "    \t\trace.append(left_txt[7].get_text().strip())\n",
        "    \t\theight.append(left_txt[9].get_text().strip())\n",
        "    \t\tweight.append(left_txt[11].get_text().strip())\n",
        "    \t\teye_col.append(left_txt[13].get_text().strip())\n",
        "    \t\thair_col.append(left_txt[15].get_text().strip())\n",
        "    \t\tarrest_agency.append(left_txt[17].get_text().strip())\n",
        "    \t\tarrest_date.append(left_txt[19].get_text().strip())\n",
        "    \t\toffense1.append(left_txt[20].get_text().strip())\n",
        "    \t\toffense2.append(left_txt[27].get_text().strip())\n",
        "    \t\toffense3.append(left_txt[34].get_text().strip())\n",
        "    \t\toffense4.append(left_txt[41].get_text().strip())\n",
        "    \t\tpin.append(left_txt[49].get_text().strip())\n",
        "    \t\tlocation.append(left_txt[51].get_text().strip())\n",
        "    \t\tincrement += 1\n",
        "\n",
        "    \texcept IndexError:\n",
        "    \t\tprint(\"Hinds - Index error at \", str(increment+1), len(inmate_link))\n",
        "    \t\tincrement += 1\n",
        "\n",
        "    dic = {\"Name\": name, \"Address\": address, \"DoB\": dob, \"Sex\": sex, \"Race\": race, \"Height\": height, \"Weight\": weight, \"Eye color\": eye_col,\n",
        "    \"Hair color\": hair_col, \"Arresting agency\": arrest_agency, \"Arrest date\": arrest_date, \"Pin\": pin, \"Location\": location,\n",
        "    \"First offense\": offense1, \"Second offense\": offense2, \"Third offense\": offense3, \"Fourth offense\": offense4}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/CLEAN/' + today_date + \"/\" + today_date + '_Hinds.csv', index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GrCjhOnazV0v"
      },
      "source": [
        "# @markdown Jackson Script\n",
        "\n",
        "def jackson():\n",
        "    # regex formatting to get Calculate total Bond\n",
        "    regex = re.compile('Bond:\\ \\$[0-9]*.[0-9]*')\n",
        "    regex_money = re.compile('\\$[0-9]*.[0-9]*')\n",
        "    regex_num = re.compile('[0-9]*.[0-9]*')\n",
        "\n",
        "    # obtain the total count of inmates\n",
        "    count_url = \"https://services.co.jackson.ms.us/jaildocket/_inmateList.php?Function=count\"\n",
        "    uClient = uReq(count_url)\n",
        "    count_html = uClient.read()\n",
        "    uClient.close()\n",
        "    total_count = soup(count_html, \"html.parser\")\n",
        "    print(\"Jackson - Total Inmate Count:\", total_count )\n",
        "\n",
        "    # to state details of inmates\n",
        "    inmates = {}\n",
        "    inmate_ID_list = []\n",
        "    page = 0\n",
        "    y = []\n",
        "    # Iterate through the pages of inmates and get all the inmate IDs\n",
        "    while(len(y)>0 or page == 0): # increase the page count\n",
        "        page = page + 1\n",
        "        inmate_ID = \"https://services.co.jackson.ms.us/jaildocket/_inmateList.php?Function=list&Page=\" + str(page)\n",
        "        uClient = uReq(inmate_ID)\n",
        "        inmate_ID = uClient.read()\n",
        "        uClient.close()\n",
        "        y = json.loads(soup(inmate_ID, \"html.parser\").prettify())\n",
        "        for i in y:\n",
        "            inmate_ID_list.append(i['ID_Number'].strip() )\n",
        "            for k in range(10):\n",
        "                del i[str(k)]\n",
        "            del i['RowNum']\n",
        "            del i['Name_Suffix']\n",
        "            inmates[i['ID_Number'].strip()] = i\n",
        "    print(\"Jackson - Total Count of ID Numbers Obtained:\", len(inmate_ID_list))\n",
        "    print(\"Jackson - # of Pages of inmates on website:\", page)\n",
        "\n",
        "\n",
        "    bond_count = 0\n",
        "    bondable_count = 0\n",
        "    # iterate through inmate cards with the inmate IDs and store in inmates dict\n",
        "    for inmate_ID in inmate_ID_list:\n",
        "        try:\n",
        "            my_url ='https://services.co.jackson.ms.us/jaildocket/inmate/_inmatedetails.php?id='+ inmate_ID\n",
        "            # opening up connection, grapping the page\n",
        "            uClient = uReq(my_url)\n",
        "            page_html = uClient.read()\n",
        "            uClient.close()\n",
        "            page_soup = soup(page_html, \"html.parser\")\n",
        "\n",
        "            # Obtain inmate details (race, height, ... whether they are bondable )\n",
        "            container = page_soup.select(\"[class~=iltext] p\")\n",
        "            name = []\n",
        "            bondable = \"No\"\n",
        "            for i in container:\n",
        "                item = ' '.join(i.string.split())\n",
        "                if item == 'Bondable':\n",
        "                    bondable_count = bondable_count +1\n",
        "                    bondable = \"Yes\"\n",
        "                name.append(item)\n",
        "\n",
        "            # Obtain their offense charge and bond amount\n",
        "            container = page_soup.select(\"[class~=offenseItem] p\")\n",
        "            offense = []\n",
        "            for i in container:\n",
        "                item = ' '.join(i.string.split())\n",
        "                offense.append(item)\n",
        "\n",
        "            # Calculate the total bond amount for the inmate\n",
        "            total = 0\n",
        "            bonds = regex_money.findall(str(regex.findall(str(offense))))\n",
        "            for b in bonds:\n",
        "                total = total + Decimal(sub(r'[^\\d.]', '', b))\n",
        "\n",
        "            # Store all values in dictionary for the inmate\n",
        "            inmates[inmate_ID][\"Total Bond($)\"] = total\n",
        "            inmates[inmate_ID][\"Bondable?\"] = bondable\n",
        "            inmates[inmate_ID][\"inmate_info\"] = name\n",
        "            inmates[inmate_ID][\"inmate_offense\"] = offense\n",
        "\n",
        "            # Calculate the amount of inmates that are Bondable\n",
        "            if inmates[inmate_ID][\"Total Bond($)\"]>0:  bond_count = bond_count + 1\n",
        "\n",
        "        except:\n",
        "            print(inmate_ID)\n",
        "\n",
        "\n",
        "    print(\"Jackson - # of inmates with bond:\", bond_count)\n",
        "    print(\"Jackson - # of inmates that are bondable:\", bondable_count)\n",
        "\n",
        "    # Store Values in CSV\n",
        "    csv_columns = list(list(inmates.values())[0].keys())\n",
        "    dict_data = list(inmates.values())\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dict_data)\n",
        "    df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/CLEAN/' + today_date + \"/\" + today_date + \"_Jackson.csv\", index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "lIB-JkvrzOrR"
      },
      "source": [
        "# @markdown Jones Script\n",
        "\n",
        "def jones():\n",
        "    #initializing fields\n",
        "    Fname, Lname, bookNum, ages, gender, race, addresses, agency, book_date, charges, bonds, bondable = ([] for i in range(12))\n",
        "    num_bondable = 0\n",
        "\n",
        "    u_jones = requests.get(\"https://www.jonesso.com/roster.php\")\n",
        "    jones_soup = soup(u_jones.text, \"html.parser\")\n",
        "    u_jones.close()\n",
        "\n",
        "    num_inmates = int(jones_soup.find_all(\"h2\")[0].get_text().replace(\"Inmate Roster (\", \"\").replace(\")\", \"\").strip())\n",
        "    num_pages = math.ceil(num_inmates/20)\n",
        "    page_range = range(num_pages+1)[1:]\n",
        "    inmate_links = []\n",
        "    page_prefix = \"https://www.jonesso.com/roster.php?&grp=\"\n",
        "    inmate_prefix = \"https://www.jonesso.com/\"\n",
        "\n",
        "\n",
        "    for page in page_range:\n",
        "    \t# print(page, page_range)\n",
        "    \tu_page = requests.get(page_prefix + str(page*20))\n",
        "    \tpage_soup = soup(u_page.text, \"html.parser\")\n",
        "    \tu_page.close()\n",
        "\n",
        "\n",
        "    \ta_tags = (page_soup.find_all(attrs={\"id\":\"cms-body-content\"})[0].find_all(\"a\"))\n",
        "    \tfor a in a_tags:\n",
        "    \t\tinmate_links.append(a.get(\"href\"))\n",
        "\n",
        "\n",
        "    i=0\n",
        "    for inmate in inmate_links:\n",
        "    \ti+=1\n",
        "    \tcgs = []\n",
        "    \tu_inmate = requests.get(inmate_prefix + inmate)\n",
        "    \tinmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "    \tu_inmate.close()\n",
        "\n",
        "    \tname = inmate_soup.select(\".ptitles\")[0].get_text().split()\n",
        "    \tFname.append(name[0])\n",
        "    \tLname.append(name[1])\n",
        "\n",
        "    \trows = inmate_soup.find(attrs={\"id\":\"cms-body-content\"}).select(\".row\")[0].select(\".row\")\n",
        "    \tbookNum.append(rows[0].find_all(\"div\")[1].get_text())\n",
        "    \tages.append(rows[1].get_text().strip().strip(\"Age:\").strip())\n",
        "    \tgender.append(rows[2].get_text().strip().strip(\"Gender:\").strip())\n",
        "    \trace.append(rows[3].get_text().strip().strip(\"Race:\").strip())\n",
        "    \taddresses.append(rows[4].get_text().strip().strip(\"Address:\").strip())\n",
        "    \tagency.append(rows[5].get_text().strip(\"Arresting Agency:\").strip())\n",
        "    \tbook_date.append(rows[6].get_text().replace(\"Booking Date:\", \"\").strip().split()[0])\n",
        "    \toff_str = str(rows[8]).strip(\"<div class=\\\"row\\\">\").strip().strip(\"<div class=\\\"cell inmate_profile_data_content\\\"><span class=\\\"text2\\\">\")\n",
        "    \toff_str = off_str.replace(\"<br/></span></div>\\n</\", \"\").strip().split(\"<br/>\")\n",
        "    \tfor off in off_str:\n",
        "    \t\tcgs.append(off.strip())\n",
        "    \tcharges.append(cgs)\n",
        "    \tbond = int(rows[9].get_text().strip().strip(\"Bond:\").replace(\"$\", \"\").strip())\n",
        "    \tbonds.append(bond)\n",
        "    \tif bond != 0:\n",
        "    \t\tbondable.append(True)\n",
        "    \t\tnum_bondable += 1\n",
        "    \telse:\n",
        "    \t\tbondable.append(False)\n",
        "\n",
        "    dic = {\"First name\": Fname, \"Last name\": Lname, \"Booking Number\": bookNum, \"Age\": ages, \"Gender\": gender,\n",
        "    \t\"Race\": race, \"Address\": addresses, \"Arrest agency\": agency, \"Book date\": book_date, \"Charges\": charges,\n",
        "    \t\"Bond\": bonds, \"Bondable?\": bondable}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    print(\"Jones - There are currently \", num_bondable, \" bondable detainees.\")\n",
        "    df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/CLEAN/' + today_date + \"/\" + today_date + '_Jones.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "afzz9Al6y_jE"
      },
      "source": [
        "# @markdown Kemper Script\n",
        "\n",
        "def kemper():\n",
        "    first_names, middle_names, last_names, booking_nums, ages, gender, race, addresses, book_dates, charges = ([] for i in range(10))\n",
        "\n",
        "    u_Kemper = requests.get(\"https://www.kempercountysheriff.com/roster.php?&grp=10\")\n",
        "    kemper_soup = soup(u_Kemper.text, \"html.parser\")\n",
        "    u_Kemper.close()\n",
        "\n",
        "    #scrapes the first page to get the number of inmates, computes the number of pages, then generates links\n",
        "    page_links = []\n",
        "    inmate_links = []\n",
        "    num_inmates = int(kemper_soup.h2.get_text().strip(\"Inmate Roster (\").strip(\")\"))\n",
        "    num_pages = math.ceil(num_inmates/10)\n",
        "\n",
        "    for n in range(num_pages):\n",
        "    \tpage_links.append(\"https://www.kempercountysheriff.com/roster.php?&grp=\" + str((n+1)*10))\n",
        "\n",
        "    #scrapes each page for inmate links\n",
        "    for page in page_links:\n",
        "    \tu_page = requests.get(page)\n",
        "    \tpage_soup = soup(u_page.text, \"html.parser\")\n",
        "    \tu_page.close()\n",
        "\n",
        "    \tinmate_table = page_soup.find_all(attrs={\"id\":\"cms-body-content\"})[0].find_all(\"a\")\n",
        "    \tfor inmate in inmate_table:\n",
        "    \t\tinmate_links.append(\"https://www.kempercountysheriff.com/\" + inmate.get(\"href\"))\n",
        "\n",
        "    i=0\n",
        "    for inmate in inmate_links:\n",
        "    \ti = i+1\n",
        "    \tu_inmate = requests.get(inmate)\n",
        "    \tinmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "    \tu_inmate.close()\n",
        "\n",
        "    \tname = inmate_soup.select(\".ptitles\")[0].get_text().split()\n",
        "    \tfirst_names.append(name[0])\n",
        "    \tlast_names.append(name[-1])\n",
        "    \tmiddle_names.append(name[1:-1])\n",
        "\n",
        "    \trows = inmate_soup.find_all(attrs={\"id\":\"cms-body-content\"})[0].select(\".row\")\n",
        "    \t#note that not all pages have the address listed, this messes up indexing\n",
        "    \tbooking_nums.append(rows[1].find_all(\"div\")[1].get_text())\n",
        "    \tages.append(rows[2].get_text().replace(\"Age:\", \"\").strip())\n",
        "    \tgender.append(rows[3].get_text().replace(\"Gender:\", \"\").strip())\n",
        "    \trace.append(rows[4].get_text().replace(\"Race:\", \"\").strip())\n",
        "    \tif len(rows) == 9:\n",
        "    \t\taddresses.append(\"\")\n",
        "    \t\tbook_dates.append(rows[5].get_text().split()[2])\n",
        "    \t\tcharges.append(rows[7].get_text().strip())\n",
        "\n",
        "    \telse:\n",
        "    \t\taddresses.append(rows[5].get_text().replace(\"Address:\", \"\").strip())\n",
        "    \t\tbook_dates.append(rows[6].get_text().split()[2])\n",
        "    \t\tcharges.append(rows[8].get_text().strip())\n",
        "\n",
        "    dic = {\"First name\": first_names, \"Middle name\": middle_names, \"Last name\": last_names,\n",
        "    \t\"Booking number\": booking_nums, \"Age\": ages, \"Gender\": gender, \"Race\": race,\n",
        "    \t\"Address\": addresses, \"Booking date\": book_dates, \"Charges\": charges}\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/CLEAN/' + today_date + \"/\" + today_date + '_Kemper.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vefx1gAay3OB"
      },
      "source": [
        "# @markdown Madison Script\n",
        "\n",
        "def madison():\n",
        "    #opening connection, reading page\n",
        "    u_Madison = urllib.request.urlopen('http://mydcstraining.com/agencyinfo/MS/4360/inmate/ICURRENT.HTM')\n",
        "    Madison_html = u_Madison.read()\n",
        "    u_Madison.close()\n",
        "    mad_soup = soup(Madison_html, \"html.parser\")\n",
        "    prefix = 'http://mydcstraining.com/agencyinfo/MS/4360/inmate/ICUD'\n",
        "\n",
        "    #determines the number of people detained\n",
        "    inmate_num = int(mad_soup.find_all('b')[4].get_text())\n",
        "\n",
        "    #generates a list of links to intake reports\n",
        "    links = []\n",
        "    for i in range(inmate_num):\n",
        "    \tif i < 9:\n",
        "    \t\tlinks.append(prefix + \"000\" + str(i+1) + \".HTM\")\n",
        "    \telif i < 99:\n",
        "    \t\tlinks.append(prefix + \"00\" + str(i+1) + \".HTM\")\n",
        "    \telse:\n",
        "    \t\tlinks.append(prefix + \"0\" + str(i+1) + \".HTM\")\n",
        "\n",
        "    #initializes lists\n",
        "    #note that the index is offset by one from the list of inmates in the url (starts at 1 rather than zero)\n",
        "    names, ages, birthdays, arrest_dates, heights, weights, race, sex, hair_colors = ([] for i in range(9))\n",
        "    eye_colors, facial_hair, complexions, off_dates, case_nums, intake_dates, intake_times = ([] for i in range(7))\n",
        "    intake_nums, arrest_agencies, written_bonds, cash_bonds, bondable, bondable_names, offenses = ([] for i in range(7))\n",
        "    num_bondable = 0\n",
        "    index = 0\n",
        "\n",
        "\n",
        "    for link in links:\n",
        "    \tu_inmate = urllib.request.urlopen(link)\n",
        "    \tinmate_html = u_inmate.read()\n",
        "    \tu_inmate.close()\n",
        "    \tinmate_soup = soup(inmate_html, \"html.parser\")\n",
        "    \tfieldsets = inmate_soup.find_all(\"fieldset\")\n",
        "\n",
        "    \t#Finds names and splits among respective lists according to length\n",
        "    \t#note that this produces an array for the first name when there are three names, is this an issue?\n",
        "    \tname_dob_age = fieldsets[0].find_all(\"tr\")\n",
        "    \tname = name_dob_age[0].get_text()[9:]\n",
        "    \tnames.append(name)\n",
        "\n",
        "    \t#finds age and birthday from same fieldset as name\n",
        "    \tdob_age = name_dob_age[3].find_all(\"td\")\n",
        "    \tbirthdays.append(dob_age[0].get_text()[5:].strip())\n",
        "    \tages.append(dob_age[1].get_text()[5:].strip())\n",
        "\n",
        "    \t#scrapes the \"description\" box\n",
        "    \tappearance_fields = fieldsets[1].find_all(\"tr\")\n",
        "    \theights.append(appearance_fields[0].find_all(\"td\")[0].get_text()[5:].strip())\n",
        "    \tweights.append(appearance_fields[0].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "    \trace.append(appearance_fields[1].find_all(\"td\")[0].get_text()[6:].strip())\n",
        "    \tsex.append(appearance_fields[1].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "    \thair_colors.append(appearance_fields[2].find_all(\"td\")[0].get_text()[6:].strip())\n",
        "    \teye_colors.append(appearance_fields[2].find_all(\"td\")[1].get_text()[5:].strip())\n",
        "    \tfacial_hair.append(appearance_fields[3].find_all(\"td\")[0].get_text()[7:].strip())\n",
        "    \tcomplexions.append(appearance_fields[3].find_all(\"td\")[1].get_text()[7:].strip())\n",
        "\n",
        "    \t#scrapes the \"intake/booking\" box\n",
        "    \tintake_fields = fieldsets[2].find_all(\"tr\")\n",
        "    \toff_date = intake_fields[0].get_text()[11:].split()[0]\n",
        "    \tif off_date == \"00/00/0000\":\n",
        "    \t\toff_date = \"\"\n",
        "    \toff_dates.append(off_date)\n",
        "    \tcase_num = intake_fields[0].get_text()[11:].split()[3]\n",
        "    \tif case_num == \"OTHER\":\n",
        "    \t\tcase_num = \"\"\n",
        "    \tcase_nums.append(case_num)\n",
        "\n",
        "    \tintake_dates.append(intake_fields[1].get_text().split()[2])\n",
        "    \tintake_times.append(intake_fields[1].get_text().split()[4])\n",
        "    \tintake_nums.append(intake_fields[2].get_text()[11:].strip())\n",
        "    \tarrest_agencies.append(intake_fields[3].get_text()[19:].strip())\n",
        "\n",
        "    \t#Bond Information box\n",
        "    \twritten_bond = (float(fieldsets[4].find_all(\"tr\")[0].get_text().split()[3].strip(\",grt\").replace(',', \"\")))\n",
        "    \twritten_bonds.append(written_bond)\n",
        "    \tbondable_ = bool(written_bond)\n",
        "    \tbondable.append(bondable_)\n",
        "    \tif bondable_:\n",
        "    \t\tnum_bondable += 1\n",
        "    \t\tbond_names = [name.strip(), index]\n",
        "    \t\tbondable_names.append(bond_names)\n",
        "    \tcash_bonds.append(float(fieldsets[4].find_all(\"tr\")[1].get_text().split()[3].strip(\",grt\").replace(',', \"\")))\n",
        "\n",
        "    \t#Offense text box\n",
        "    \toffs = fieldsets[5].find_all(\"tr\")[2:]\n",
        "    \tdesc = []\n",
        "    \tcourt = []\n",
        "    \tbond_amt_ = []\n",
        "    \tfor o in offs:\n",
        "    \t\toff = o.find_all(\"td\")\n",
        "    \t\tdesc.append(off[0].get_text())\n",
        "    \t\tcourt.append(off[1].get_text().strip() + \" COURT\")\n",
        "    \t\tbond_amt = off[2].get_text().strip().replace(\",\", \"\")\n",
        "    \t\tif bond_amt == \"\":\n",
        "    \t\t\tbond_amt = \"0\"\n",
        "    \t\tbond_amt_.append(float(bond_amt.strip(\"$s\")))\n",
        "\n",
        "    \toff_dict = {\"Description\": desc, \"Court\": court, \"Bond amount\": bond_amt_}\n",
        "    \toffenses.append(off_dict)\n",
        "    \tindex += 1\n",
        "\n",
        "    #generates a dataframe with dictionary dic, then prints dataframe as specified\n",
        "    dic = {\"Name\": names, \"Ages\": ages, \"Birthdays\": birthdays,\n",
        "    \t\"Height\": heights, \"Weight\": weights, \"Race\": race, \"Sex\": sex, \"Hair color\": hair_colors, \"Eye color\": eye_colors, \"Facial hair\": facial_hair,\n",
        "    \t\"Complexion\": complexions,\n",
        "    \t\"Off date\": off_dates, \"Case number\": case_nums, \"Intake date\": intake_dates, \"Intake number\": intake_nums, \"Arresting agency\": arrest_agencies,\n",
        "    \t\"Bondable?\": bondable, \"Written bond\": written_bonds, \"Cash bond\": cash_bonds,\n",
        "    \t\"Offense(s)\": offenses}\n",
        "    #make offenses a dictionary so it's more readable\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/CLEAN/' + today_date + \"/\" + today_date + '_Madison.csv', index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "myLQsez6ytvd"
      },
      "source": [
        "# @markdown Tunica Script\n",
        "def tunica():\n",
        "    first_names, middle_names, last_names, booking_nums, ages, gender, race = ([] for i in range(7))\n",
        "    arrest_agency, book_date, charges, bonds, bondable, page_links, inmate_links  = ([] for i in range(7))\n",
        "    num_bondable = 0\n",
        "\n",
        "    u_Tunica = requests.get(\"https://www.tunicamssheriff.com/roster.php\")\n",
        "    tunica_soup = soup(u_Tunica.text, \"html.parser\")\n",
        "    u_Tunica.close()\n",
        "\n",
        "    page_prefix = \"https://www.tunicamssheriff.com/roster.php?&grp=\"\n",
        "    inmate_prefix = \"https://www.tunicamssheriff.com/\"\n",
        "    num_inmates = int(tunica_soup.select(\".ptitles\")[0].get_text()[15:-1])\n",
        "    num_pages = math.ceil(num_inmates/10)\n",
        "    for p in range(num_pages):\n",
        "    \tpage_links.append(page_prefix + str((p+1)*10))\n",
        "\n",
        "    for page in page_links:\n",
        "    \tu_page = requests.get(page)\n",
        "    \tpage_soup = soup(u_page.text, \"html.parser\")\n",
        "    \tu_page.close()\n",
        "\n",
        "    \tlinks = page_soup.find_all(attrs={\"id\":\"cms-body-content\"})[0].select(\"a\")\n",
        "    \tfor link in links:\n",
        "    \t\tinmate_links.append(inmate_prefix + link.get(\"href\"))\n",
        "\n",
        "    for inmate in inmate_links:\n",
        "    \ttry:\n",
        "    \t\tu_inmate = requests.get(inmate)\n",
        "    \t\tinmate_soup = soup(u_inmate.text, \"html.parser\")\n",
        "    \t\tu_inmate.close()\n",
        "\n",
        "    \t\tname = (inmate_soup.select(\".ptitles\")[0].get_text().split())\n",
        "    \t\trows = inmate_soup.find_all(attrs={\"id\":\"cms-body-content\"})[0].select(\".row\")[0].select(\".row\")\n",
        "\n",
        "    \t\tbooking_num = int(rows[0].find_all(\"div\")[1].get_text())\n",
        "    \t\tage = int(rows[1].find_all(\"div\")[1].get_text())\n",
        "    \t\tgender_person = rows[2].find_all(\"div\")[1].get_text()\n",
        "    \t\trace_person=rows[3].find_all(\"div\")[1].get_text()\n",
        "    \t\tarrestAgency=rows[4].find_all(\"div\")[1].get_text()\n",
        "    \t\tbookDate=rows[5].find_all(\"div\")[1].get_text().split()[0]\n",
        "    \t\toffs_str = (str(rows[7].select(\".text2\")[0]).replace(\"<span class=\\\"text2\\\">\", \"\").replace(\"</span>\", \"\").split(\"<br/>\"))\n",
        "\n",
        "    \t\tfirstname=name[0]\n",
        "    \t\tlastname=name[-1]\n",
        "    \t\tmiddlename=name[1:-1]\n",
        "\n",
        "    \t\tfirst_names.append(firstname)\n",
        "    \t\tlast_names.append(lastname)\n",
        "    \t\tmiddle_names.append(middlename)\n",
        "    \t\tbooking_nums.append(booking_num)\n",
        "    \t\tages.append(age)\n",
        "    \t\tgender.append(gender_person)\n",
        "    \t\trace.append(race_person)\n",
        "    \t\tarrest_agency.append(arrestAgency)\n",
        "    \t\tbook_date.append(bookDate)\n",
        "\n",
        "    \t\toffs = []\n",
        "    \t\tfor off in offs_str:\n",
        "    \t\t\toffs.append(off.strip())\n",
        "    \t\tcharges.append(offs)\n",
        "    \t\ttry:\n",
        "    \t\t\tbd = rows[8].get_text().replace(\"Bond:\", \"\").strip()\n",
        "    \t\t\tif \"$\" in bd:\n",
        "    \t\t\t\tbonds.append(float(bd.strip(\"$\")))\n",
        "    \t\t\t\tbondable.append(True)\n",
        "    \t\t\t\tnum_bondable += 1\n",
        "    \t\t\telse:\n",
        "    \t\t\t\tbonds.append(0.0)\n",
        "    \t\t\t\tbondable.append(False)\n",
        "    \t\texcept IndexError:\n",
        "    \t\t\tbonds.append(0.0)\n",
        "    \t\t\tbondable.append(False)\n",
        "    \texcept:\n",
        "    \t\tcontinue\n",
        "\n",
        "    dic = {\"First name\": first_names, \"Middle name\": middle_names, \"Last name\": last_names,\n",
        "    \t\"Booking number\": booking_nums, \"Age\": ages, \"Gender\": gender, \"Race\": race,\n",
        "    \t\"Arrest agency\": arrest_agency, \"Booking date\": book_date, \"Charges\": charges,\n",
        "    \t\"Bond\": bonds, \"Bondable?\": bondable}\n",
        "\n",
        "    df = pd.DataFrame.from_dict(dic)\n",
        "    df.to_csv('/content/drive/MyDrive/Nithi-Thesis_Bail-Project/scraped_files/CLEAN/' + today_date + \"/\" + today_date + '_Tunica.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Y_EaNPdCI9U"
      },
      "source": [
        "\n",
        "class myThread (threading.Thread):\n",
        "   def __init__(self, threadID, townName, validate_r):\n",
        "      threading.Thread.__init__(self)\n",
        "      self.threadID = threadID\n",
        "      self.townName = townName\n",
        "      self.validate_r = validate_r\n",
        "   def run(self):\n",
        "      print (\"Starting \" + self.townName)\n",
        "      if self.townName in towns:\n",
        "        townScraper(self.townName, self.validate_r)\n",
        "      elif self.townName == 'PearlRiver':\n",
        "        townScraperPearl()\n",
        "      elif self.townName == \"clay\":\n",
        "          clay()\n",
        "      elif self.townName == 'adams':\n",
        "          adams()\n",
        "      elif self.townName == 'hinds':\n",
        "          hinds()\n",
        "      elif self.townName == 'jackson':\n",
        "          jackson()\n",
        "      elif self.townName == 'jones':\n",
        "          jones()\n",
        "      elif self.townName == 'kemper':\n",
        "          kemper()\n",
        "      elif self.townName == 'madison':\n",
        "          madison()\n",
        "      elif self.townName == 'tunica':\n",
        "          tunica()\n",
        "\n",
        "      print (\"Exiting \" + self.townName)\n",
        "\n",
        "threads = []\n",
        "threadID = 1\n",
        "\n",
        "towns = list(jail_captcha.keys())\n",
        "\n",
        "Create new threads\n",
        "for townName in towns:\n",
        "  print(\"Create new Thread\", townName)\n",
        "  # get captcha image and enter in captcha information to get validation key\n",
        "  captchaMatched = False\n",
        "  while (not captchaMatched):\n",
        "    captcha_r = requests.get('https://omsweb.public-safety-cloud.com/jtclientweb/captcha/getnewcaptchaclient')\n",
        "    captchaKey = captcha_r.json()['captchaKey']\n",
        "    image = captcha_r.json()['captchaImage']\n",
        "    html = f'<img src=\"{image}\"/>'\n",
        "    display.display(display.HTML(html))\n",
        "    userCode = input()\n",
        "\n",
        "    jail_captcha[townName]['userCode'] = str(userCode)\n",
        "    validate_r = requests.post( \n",
        "        'https://omsweb.public-safety-cloud.com/jtclientweb/Captcha/validatecaptcha',\n",
        "        json={'userCode': jail_captcha[townName]['userCode'] , 'captchaKey': captchaKey}\n",
        "    )\n",
        "    captchaMatched = validate_r.json()['captchaMatched']\n",
        "\n",
        "  thread = myThread(threadID, townName, validate_r)\n",
        "  threads.append(thread)\n",
        "  threadID += 1\n",
        "\n",
        "# for townName in ['PearlRiver', 'clay', 'adams', 'hinds', 'jackson', 'jones', 'kemper', 'madison', 'tunica']:\n",
        "for townName in ['clay', 'adams', 'hinds', 'jackson', 'jones', 'kemper', 'madison', 'tunica']:\n",
        "  thread = myThread(threadID, townName, 'None')\n",
        "  threads.append(thread)\n",
        "  threadID += 1\n",
        "\n",
        "for t in threads:\n",
        "   t.start()\n",
        "\n",
        "# Wait for all threads to complete\n",
        "for t in threads:\n",
        "   t.join()\n",
        "\n",
        "print(\"Exiting Main Thread\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}